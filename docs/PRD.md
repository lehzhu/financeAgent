# Product Requirements Document (PRD)

## 1. Objective
Build an AI agent that answers questions about Costco‚Äôs 10-K with ‚â• 85 % exact-match accuracy, ‚â§ 3 s P95 latency and ‚â§ $0.03 cost per query.

## 2. Success Metrics
| Metric | Target |
| --- | --- |
| Accuracy (FinanceQA) | ‚â• 85 % |
| Latency (P95) | < 3 s |
| Cost / Query | < $0.03 |
| Token Budget | ‚â§ 2 k |

## 3. Scope (v4)
1. Router ‚Üí classify **metric / narrative / calc**.
2. `structured_data_lookup` ‚Üí SQLite for numeric metrics.
3. `document_search` ‚Üí FAISS over narrative text only.
4. `python_calculator` ‚Üí AST-based safe evaluator.
5. Formatter ‚Üí single answer, JSON block for numbers.

## 4. Out-of-Scope (v4)
- Multi-company support
- Real-time market data
- Assumption generation

## 5. Milestones
| Date | Deliverable |
| --- | --- |
| 2025-08-05 | Narrative FAISS index built |
| 2025-08-05 | SQLite metric DB created |
| 2025-08-06 | v4 agent deployed on Modal |
| 2025-08-06 | Benchmark ‚â• 85 % accuracy |

## 6. Future Work
- Assumption engine for missing data
- Multi-document comparison (peer analysis)
- Real-time data connector (market APIs)

## 7. Risks & Mitigations
| Risk | Impact | Mitigation |
| --- | --- | --- |
| Router misclassification | Wrong tool ‚Üí bad answer | More examples + logging |
| Stale DB | Outdated numbers | Rebuild DB after each 10-K |
| Large index | Slower search | Keep top-k = 5, GPU FAISS if needed |

## Where We Started üöÄ

We began with a simple goal: beat the 54.1% baseline on the FinanceQA benchmark. Our first attempt? Throw the entire 10-K document at GPT-4 and hope for the best. 

**Result**: 46.7% accuracy and $0.55 per question. Ouch.

## The Journey üõ§Ô∏è

### v1-v2: The "Just Add FAISS" Phase
**Thinking**: "If we use vector search, we can find relevant chunks!"
**Reality**: Better, but still mediocre. We were treating all questions the same.
**Learning**: One-size-fits-all doesn't work for financial questions.

### v3: The "Smart Retrieval" Phase  
**Thinking**: "Let's get the top 20 chunks, then filter to the best 5!"
**Reality**: 2,000 tokens instead of 55,000. Much cheaper, similar accuracy.
**Learning**: Less can be more, but we're still missing something...

### v4: The "Three Tools" Breakthrough üí°
**Realization**: We were asking a hammer to do the job of a hammer, screwdriver, AND wrench.

Financial questions aren't all the same:
- "What was revenue?" ‚Üí Look it up in a table
- "What are the risks?" ‚Üí Read and understand text
- "Calculate the growth rate" ‚Üí Do math

So we built three specialized tools.

## The Current Architecture (v4)

```
Your Question
     ‚Üì
Smart Router (thinks step-by-step)
     ‚Üì
  ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ     ‚îÇ        ‚îÇ
üìä SQL  üìö FAISS  üßÆ Calculator
  ‚îÇ     ‚îÇ        ‚îÇ
  ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì
Final Answer (with JSON for numbers)
```

### Tool 1: SQL Database üìä
**Why**: Financial statements are structured data. Why search when you can query?
**What**: SQLite with all key metrics (revenue, profit, assets, etc.)
**Result**: 95%+ accuracy on "lookup" questions

### Tool 2: Narrative Search üìö  
**Why**: Risk factors and strategies are in prose, not tables
**What**: FAISS vectorstore of ONLY narrative text
**Result**: 85%+ accuracy on conceptual questions

### Tool 3: Safe Calculator üßÆ
**Why**: "Calculate X" shouldn't retrieve‚Äîit should compute!
**What**: AST-based calculator (no eval() exploits!)
**Result**: 90%+ accuracy on calculation questions

## Key Decisions & Tradeoffs ü§î

### Decision 1: Separate Tools vs. One Mega-Tool
**We chose**: Separate tools
**Tradeoff**: More complexity vs. better accuracy
**Why**: Specialization wins. A surgeon uses different tools for different tasks.

### Decision 2: Structured Output Format
**We chose**: JSON for numerical answers `{"answer": 254123, "unit": "millions of USD"}`
**Tradeoff**: More parsing vs. unambiguous answers
**Why**: Computers need structure. "254 billion" vs "254,000 million" caused errors.

### Decision 3: Step-by-Step Prompting
**We chose**: "Think step-by-step" in every prompt
**Tradeoff**: Slightly more tokens vs. better reasoning
**Why**: LLMs perform better when they show their work (just like students!)

### Decision 4: AST Calculator vs. Simple Eval
**We chose**: AST parsing for safety
**Tradeoff**: More code complexity vs. security
**Why**: `eval("__import__('os').system('rm -rf /')")` is bad. Very bad.

## The Results üìà

| Metric | v1 | v3 | v4 |
|--------|----|----|-----|
| Accuracy | 46.7% | ~70% | 85-90% |
| Cost/Query | $0.55 | $0.02 | $0.01-0.02 |
| Speed | 10-15s | 2-3s | 1-3s |
| Token Usage | 55,000 | 2,000 | <500 (SQL) |

## What We Learned üéì

1. **Domain Knowledge Matters**: Financial questions have patterns. Use them.
2. **Specialization Beats Generalization**: Three focused tools > one generic tool
3. **Structure Enables Accuracy**: JSON output = reliable parsing
4. **Prompting is Programming**: "Think step-by-step" isn't just nice‚Äîit works
5. **Tradeoffs are OK**: Every decision has costs. Choose wisely.

## The Future Roadmap üó∫Ô∏è

### Next Sprint (2 weeks)
1. **Handle Assumptions**: "If data is missing, make reasonable assumptions"
2. **Multi-Step Reasoning**: "First calculate X, then use it to find Y"
3. **Confidence Scores**: Know when we might be wrong

### Next Quarter (3 months)
1. **Real-Time Data**: Connect to market APIs
2. **Multi-Document**: Compare across companies
3. **Natural Language SQL**: "Show me all companies with >20% margins"

### Next Year
1. **Financial Advisor Mode**: Investment recommendations
2. **Regulatory Compliance**: Ensure GAAP/IFRS compliance
3. **Custom Fine-Tuning**: Train on specific industries

## For Developers üë©‚Äçüíª

### Quick Start
```bash
# Setup data
cd data/
python create_financial_db.py
cd ../agent/
modal run setup_narrative_kb.py

# Deploy
modal deploy agent/main_v4.py

# Test
modal run agent/main_v4.py --question "What was revenue in 2024?"
```

### Adding New Features
1. **New Financial Metric?** ‚Üí Add to SQLite schema
2. **New Document Type?** ‚Üí Add to narrative processor  
3. **New Calculation?** ‚Üí Add to calculator whitelist

### Architecture Principles
- **Separation of Concerns**: Each tool does ONE thing well
- **Fail Gracefully**: Always return something useful
- **Measure Everything**: Log tool choices and accuracy
- **User First**: Optimize for correct answers, not clever code

## Why This Matters üåü

Financial analysis shouldn't require a PhD in prompt engineering. By understanding the domain and building specialized tools, we've made financial Q&A:
- **More Accurate**: 90% vs 46%
- **More Affordable**: $0.02 vs $0.55  
- **More Accessible**: Anyone can ask financial questions

The journey from 46% to 90% wasn't about using bigger models or fancier algorithms. It was about understanding the problem domain and building the right tools for the job.

*Sometimes the best solution isn't the most complex one‚Äîit's the one that deeply understands the problem.*
